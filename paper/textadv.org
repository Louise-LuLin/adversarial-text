#+TITLE: Adversarial Texts with Gradient Methods
#+AUTHOR: Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, Wei-Shinn Ku

#+STARTUP: overview
#+OPTIONS: toc:nil num:t ^:{} author:nil title:nil date:nil

#+LATEX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [conference,letter,10pt,final,dvipsnames]

#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage[inline]{enumitem}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage[backend=biber]{biblatex}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=basictext,factor=1100,stretch=10,shrink=10]{microtype}

#+LATEX_HEADER: \addbibresource{~/.local/data/bibliography/nn.bib}
#+LATEX_HEADER: \DeclareMathOperator{\sign}{sign}

#+MACRO: empty @@latex:@@

# Real title and author that will show

* Preface                                                            :ignore:
:PROPERTIES:
:CUSTOM_ID: h1-preface-6b330
:END:
#+BEGIN_EXPORT latex
% This is the real title appearing in the final PDF
\title{Adversarial Texts with Gradient Methods}

\author{
\IEEEauthorblockN{
  Zhitao Gong\IEEEauthorrefmark{1},
  Wenlu Wang\IEEEauthorrefmark{1},
  Bo Li\IEEEauthorrefmark{2},
  Dawn Song\IEEEauthorrefmark{2},
  Wei-Shinn Ku\IEEEauthorrefmark{1}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}
  \texttt{\{gong,wenluwang,weishinn\}@auburn.edu}\\
  Auburn University, Auburn, AL, USA}
\IEEEauthorblockA{\IEEEauthorrefmark{2}
  \texttt{\{crystalboli,dawnsong\}@berkeley.edu}\\
  University of California, Berkeley, Berkeley, CA, USA}
}
#+END_EXPORT

#+LaTeX: \maketitle

* Abstract                                                           :ignore:
:PROPERTIES:
:CUSTOM_ID: h1-abstract-f6db2
:END:

#+BEGIN_abstract
Adversarial samples in the image domain have been extensively studied in the
literature.  Among many of the adversarial generating methods, the model
gradient-based methods are both effective and easy to compute.  In this work, we
propose a framework to adapt the model gradient-based methods on images to text
domain.  The main difficulty for generating adversarial texts with
gradient-based methods is that the input space is discrete, which makes it
difficult to accumulate small noise directly in the inputs.  We tackle work
around this problem by searching for adversarials in the embedding space and
then reconstruct the adversarial texts from the noise embeddings.  We tested our
framework on two popular text classification models: word-level CNN and
character-aware CNN.  Through extensive experiments, we show that our framework
can leverage model gradient-based methods to generate very high-quality
adversarial texts.  Furthermore, we also provide a through comparison of our
framework compared with two other methods, Hotflip and DANcin seq2seq.
#+END_abstract

* Introduction
:PROPERTIES:
:CUSTOM_ID: h1-introduction-71377
:END:

The phenomenon of adversarial samples was first discussed
in cite:szegedy2013-intriguing.  The authors show that images perturbed with
carefully crafted noise may trick the deep neural networks into wrong
predictions with very high confidence.  There has been an abundance of followup
work on methods to generate adversarial images.  This adversarial phenomenon
arouses great interest among researchers since it is of great importance both in
practice and in theory.  On the one hand, these adversarial samples undermines
the reliability of deep models.  It seems that deep models may fail unexpected
in some conditions we struggle to understand.  This would raise concerns about
the application of deep models to some critical areas, e.g., self-driving cars.
On the other hand, we assume implicitly /good local generalization/ when our
model generalizes well to test data cite:szegedy2013-intriguing.  However, some
work empirically show that adversarial samples may exist in dense regions around
the clean ones, which seems to contradicts the aforementioned hypothesis.
Further study of this phenomenon, both empirically and theoretically, will help
us understand more about the dynamics of deep models.

The adversarial images have been extensively studied.  Many adversarial
generating methods have been proposed in literature, e.g, fast gradient method
(FGM) cite:goodfellow2014-explaining, Jacobian-based saliency map approach
(JSMA) cite:papernot2015-limitations,
DeepFool cite:moosavi-dezfooli2015-deepfool, CW cite:carlini2016-towards, etc.
Many theoretical explanation of adversarial samples also focused on image data
and architectures cite:peck2017-lower,goodfellow2014-explaining.  Some work have
expanded the study to other domains, e.g, speech-to-text cite:carlini2018-audio,
neural translation cite:zhao2017-generating, reinforcement
learning cite:lin2017-tactics, etc.  These extended work will give us a more
thorough understanding of the adversarial samples.  To this end, we propose a
simple yet effective framework to adapt the adversarial methods for images to
text domain.  Specifically, we focus on adversarial samples for text
classification models.  There are two major difficulties to generate adversarial
texts:
1. The input space is discrete.  As a result, it is unclear how to (iteratively)
   accumulate small noise to perturb the input.  Working with Image domain is
   easier since we usually normalize the input to \([0, 1]\).
2. The text quality measurement is intricate in itself.  It is a very subjective
   matter.  For example, let's compare the Master Yoda-style way of speaking,
   /Much to learn, you still have/, with the mundane-style, /You still have much
   to learn/.  Which is better?  Which get a high score?  Start Wars fans will
   definitely favor the Yoda-style, although both sentences successfully convey
   the exactly same meaning.

To solve the first problem, we propose a general framework in which we generate
adversarial texts via slightly modified methods borrowed from image domain.  We
first search for adversarials in the text embedding space (e.g., word-level
embedding cite:mikolov2013-efficient, character-level
embedding cite:kim2015-character), and then reconstruct the adversarial texts
with nearest neighbor search.  The second problem is open-ended, we employ two
metrics to quantify the results, i.e., the Word Mover's Distance
(WMD) cite:kusner2015-from and change ratio (number of words changed divided by
the sentence length).  In our experiments, they serve their purpose well at a
rather coarse level.  These two metrics, however, does not perform consistently
when two text pieces are about the same quality (e.g., the aforementioned
Yoda-style and mundane-style).

this paper is organized as follows.  we survey recent work on generating
adversarial images and texts in section [[ref:h1-related-work-81bde]].  a brief
review about defending against adversarials is included in
section [[ref:h1-related-work-81bde]].  our adversarial text framework is proposed
in section [[ref:h1-adversarial-text-framework-774a3]].  we thoroughly evaluate our
framework and two other methods, Hotflip cite:ebrahimi2017-hotflip and
autoencoder-based cite:zhao2017-generating on various text benchmarks and report
the results in section [[ref:h1-experiment-2f800]].  we conclude this paper and
provide directions for future work in section [[ref:h1-conclusion-2763d]].

* Related Work
:PROPERTIES:
:CUSTOM_ID: h1-related-work-81bde
:END:

The phenomenon of adversarial samples was first discussed
in cite:szegedy2013-intriguing.  Many followup work propose different methods to
generate adversarial samples.  On the other hand, many work investigate defense
methods due to the security concern raised by adversarial samples.  Generally
speaking, so far as we see in literature, the attacking is much easier and
cheaper than defense.

For notation, \(x\) denotes the input, \(y\) the prediction, \(f\) the target
model such that \(y = f(x)\), \(L\) the loss function, \(x^*\) the adversarial
sample.  \(\|\cdot\|_p\) denotes the \(p\)-norm.  We slightly abuse the notation
here, \(L_x\) denotes the loss with \(x\) as the input.

#+ATTR_LaTeX: :width \linewidth
#+CAPTION: Random MNIST adversarial images generated via different attacking algorithms.  The upper image in /Clean/ column is the original clean image.   The upper images in the following columns are adversarial images generated by the corresponding attacking algorithm based on the first clean image, respectively.  The lower image in each column is the difference between the adversarial image and the clean image, illustrated in heatmap.  Below each column is the label predicted by the target model, along with probability in parenthesis.
#+NAME: fig:mnistdemo
[[file:img/imgdemo.pdf]]

** Generate Adversarial Images
:PROPERTIES:
:CUSTOM_ID: h2-generate-adversarial-images-d136a
:END:

Generally speaking, the proposal methods fall into two strategies, the first one
is to move data points around till the label changes, and the other is to create
a mapping between clean and adversarial samples (or noises).

*** Move Data Points
:PROPERTIES:
:CUSTOM_ID: h3-move-data-points-9a4e4
:END:

Essentially, this class of methods move the data points along a carefully chosen
direction.  It has been shown that it is very unlikely to arrive at adversarial
samples following a random direction cite:szegedy2013-intriguing.
1. The direction may be where the loss for clean samples increases, e.g.,
   FGSM cite:goodfellow2014-explaining and its
   variants cite:kurakin2016-adversarial,miyato2015-distributional,kurakin2016-adversarial),
   or where the loss for adversarial samples decreases, e.g.,
   cite:szegedy2013-intriguing.
2. The direction where the probability of the correct label decreases (or the
   probabilities of the target label increases), e.g.,
   JSMA cite:papernot2015-limitations, CW cite:carlini2016-towards.
3. It could also be the direction towards the decision boundary (e.g.,
   DeepFool cite:moosavi-dezfooli2015-deepfool, one-pixel
   attack cite:su2017-one).

Fast gradient sign method (FGSM) and its variants add to the whole image the
noise that is proportional to either \(\nabla L_x\) (FGVM) or \(\sign(\nabla
L_x)\) (FGSM), which, in essence, follow the direction where the loss for the
clean samples increases.  Jacobian-based saliency map approach (JSMA), on the
contrary, perturbs one pixel at a time.  It chooses the pixel with the highest
score calculated as \(-\nabla y_t\cdot\sum\nabla y_o\) subject to \(\nabla y_t >
0\), where \(y_t\) is the probability for the target class, and \(\sum y_o\) is
the sum of probabilities of all other classes.  Intuitively, JSMA tries to
increase the probability of the target class while decreasing others.  CW
increasing the probability for wrong classes while minimizing the adversarial
noise scale.  DeepFool iteratively approximates the direction to the nearest
decision boundary.

Figure ref:fig:mnistdemo shows adversarial image examples of four gradient
methods on MNIST.  As we can see, FGSM tends to generate more salient noise
spread across the whole image.  On the other hand, FGVM is slightly better since
it uses gradients instead of the sign of gradients as noise.  In practice, most
of the absolute values of gradients are far less that 1.  JSMA, on the contrary,
increases the intensity of the most salient pixel until its value goes beyond
the input domain.  As a result, we expect to see a few very intense spots in the
image.  DeepFool, as shown in the image, usually generates very subtle noise.

*** Map Clean Samples to Adversarial
:PROPERTIES:
:CUSTOM_ID: h3-map-clean-samples-to-adversarial-b4e9b
:END:

This class of methods are relatively less explored.  Adversarial transformation
network (ATN) cite:baluja2017-adversarial employs autoencoder to generate
adversarial samples or noises.  cite:xiao2018-generating,zhao2017-generating
employs a generative model (i.e., GAN cite:goodfellow2014-generative) to map
from clean samples to adversarial ones.  The advantage of this class of methods
is that the generation is usually fast, only one pass of forward computation is
needed.

** Generate Adversarial Texts
:PROPERTIES:
:CUSTOM_ID: h2-generate-adversarial-texts-f1b71
:END:

# Should be in experiment section, placed here for typesetting.
#+BEGIN_EXPORT latex
\begin{table*}[ht]
 \caption{\label{tab:acc} Model accuracy under different parameter settings.
   \(\epsilon\) is the noise scaling factor.  We report two accuracy
   measurements per parameter setting in the format \(acc_1/acc_2\), where
   \(acc_1\) is the model accuracy on adversarial embeddings before nearest
   neighbor search, \(acc_2\) the accuracy on adversarial embeddings that are
   reconstructed by nearest neighbor search.  In other words, \(acc_2\) is the
   model accuracy on generated adversarial texts.}
\centering
\begin{tabular}{rl*{5}{c}}
  \toprule
  Method
  & Dataset
  &
  & \multicolumn{4}{c}{\(acc_1/acc_2\)} \\
  \midrule

  \multirow{5}{*}{FGSM}
  &
  & \(\epsilon\) & 0.40 & 0.35 & 0.30 & 0.25 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.1213 / 0.1334 & 0.1213 / 0.1990 & 0.1213 / 0.4074 & 0.1213 / 0.6770 \\
  & Reuters-2 & & 0.0146 / 0.6495 & 0.0146 / 0.7928 & 0.0146 / 0.9110 & 0.0146 / 0.9680 \\
  & Reuters-5 & & 0.1128 / 0.5880 & 0.1128 / 0.7162 & 0.1128 / 0.7949 & 0.1128 / 0.8462 \\
  \cmidrule(lr){1-7}

  \multirow{5}{*}{FGVM}
  &
  & \(\epsilon\) & 15 & 30 & 50 & 100 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.6888 / 0.8538 & 0.6549 / 0.8354 & 0.6277 / 0.8207 & 0.5925 / 0.7964 \\
  & Reuters-2 & &  0.7747 / 0.7990 & 0.7337 / 0.7538 & 0.6975 / 0.7156 & 0.6349 / 0.6523 \\
  & Reuters-5 & &  0.5915 / 0.7983 & 0.5368 / 0.6872 & 0.4786 / 0.6085 & 0.4000 / 0.5111\\
  \cmidrule(lr){1-7}

  \multirow{5}{*}{DeepFool}
  &
  & \(\epsilon\) & 20 & 30 & 40 & 50 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.5569 / 0.8298 & 0.5508 / 0.7225 & 0.5472 / 0.6678 & 0.5453 / 0.6416 \\
  & Reuters-2 & & 0.4416 / 0.6766 & 0.4416 / 0.5236 & 0.4416 / 0.4910 & 0.4416 / 0.4715 \\
  & Reuters-5 & & 0.1163 / 0.4034 & 0.1162 / 0.2222 & 0.1162 / 0.1641 & 0.1162 / 0.1402 \\
  \bottomrule
\end{tabular}
\end{table*}
#+END_EXPORT

To generate adversarial texts, we first need to resolve the aforementioned two
difficulties, i.e., /discrete text space/ and /text quality metrics/.  The
second problem is more subjective.  Some metrics have been proposed in
literature to quantify the text quality in different areas, e.g., BLEU
score cite:papineni2002-bleu in machine translation, Word Mover's Distance
(WMD) cite:kusner2015-from for document similarity.  As far as we know, there is
not yet a generally accepted way to quantify the adversarial texts.
Intuitively, the perturbed text pieces need to preserve the semantic meaning of
the original texts while following an /almost/ correct syntax.  In other words,
it should not be too difficult to recognize the true meaning while being able to
trick the machine.  Due to the intricacy involved in quality evaluation, we
categorize the surveyed methods by how they approach the first problem, i.e.,
the discrete text space.

*** Text-space Methods
:PROPERTIES:
:CUSTOM_ID: h3-text-space-method-e741b
:END:

This class of methods perturbs the input texts directly.  At the word level or
character level, there are three basic operations to alter the input, i.e.,
/insertion/, /deletion/, and /replacement/.  At the sentence level, a
distracting sentence may be generated or manually constructed.  On account of
preserving the semantic closeness and syntactic correctness, /replacement/ is
relatively easier since using synonyms is usually a good choice.  The other
operations will nevertheless change the structure of the sentence, albeit they
do not always hinder our understanding of the sentence.

To perturb the input texts directly, two decisions need to be made:
1. /What to change/.  Generally speaking, the words that have more influence on
   the result should be altered.  Similar to JSMA,
   cite:liang2017-deep,samanta2017-towards compute importance score for each
   word based on \(\nabla L\) or \(\nabla f\).  Words with high importance score
   are altered first.  cite:jia2017-adversarial targets a specific text model,
   i.e., the search-based QA system.  The author manually construct fake facts
   around the sentence that contains the answer.  cite:anonymous2018-adversarial
   alters the input sentence in brutal-force way, where each word is altered in
   sequence until a threshold or success.
2. /Change to what/.

*** Transformed-space Methods
:PROPERTIES:
:CUSTOM_ID: h3-transformed-space-methods-76e1e
:END:

cite:zhao2017-generating employs an autoencoder structure to map between the
input text and Gaussian noise space.  They search in the noise space for the
potential adversarial samples which are mapped back to text space to verify.
However, the is that they do not have an explicit control of the quality of the
generated adversarial samples.  As we have seen in cite:zhao2017-generating, the
generated adversarial images on complex dataset usually have large visual
changes.

In cite:liang2017-deep, the authors attempt applying FGM directly on
character-level CNN cite:zhang2015-character.  Although the labels of the text
pieces are altered, the texts are changed to basically random stream of
characters.

cite:anonymous2018-adversarial employs a brutal-force way to find perturbation.
They iteratively replace each word with its nearest neighbor in the embedding
space until success or a threshold is reached.  The computation is very
expensive.  A black-box attack based on GAN is proposed cite:wong2017-dancin.  A
highly related work is also report in cite:ebrahimi2017-hotflip where the
authors conduct character-level and word-level attack based on gradients.  The
difference is that we use nearest neighbor search to reconstruct the adversarial
sentences, while they search for adversarial candidates directly based on
certain constraints.  Thus the word-level attack was not very successfully in
cite:ebrahimi2017-hotflip.

* Adversarial Text Framework
:PROPERTIES:
:CUSTOM_ID: h1-adversarial-text-framework-774a3
:END:

In this section, we propose a general framework that generates high-quality
adversarial texts by noise generated via gradient-based methods.

** Discrete Input Space
:PROPERTIES:
:CUSTOM_ID: h2-discrete-input-space-ed243
:END:

The first problem we need to resolve is how we can accumulate small noise to
change the input.  The idea comes from the observation that the first layer for
most text models is the embedding layer.  Thus, instead of working on the raw
input texts, we first search for adversarials in the embedding space via
gradient-based methods, and then reconstruct the adversarial sentences.
Searching for adversarials in the embedding space is similar in principle to
searching for adversarial images.  However, the generated noisy embedding
vectors usually do not correspond to any tokens in the text space.  To construct
the adversarial texts, we align each embedding to its nearest one.  We can use
(approximate) nearest neighbor search if the vocabulary size is large, or direct
embedding reverse by cosine distance if the embedding matrix is relative small.
This reconstructing process can be seen as a strong /denoising/ process.  With
appropriate noise scale, we would expect most of the tokens/characters remain
unchanged, with only few replaced.  This framework builds upon the following
observations.

1. In the gradient-based methods, the input features (e.g., pixels, tokens,
   characters) that are relatively more important for the final predictions will
   receive more noise, while others relatively less noise.  The is actually the
   core property of the gradient-based methods.  For example, in
   Figure ref:fig:mnistdemo, usually only a subset of the pixels are perturbed.
2. The embedded word vectors preserve the subtle semantic relationships among
   words cite:mikolov2013-efficient,mikolov2013-distributed.  For example,
   =vec("clothing")= is closer to =vec("shirt")= as =vec("dish")= to
   =vec("bowl")=, while =vec("clothing")= is far away, in the sense of
   \(p\)-norm, from =vec("dish")= since they are not semantically
   related cite:mikolov2013-linguistic.  This property assures that it is more
   likely to replace the victim words with a semantically related one rather
   than a random one.

** Word Mover's Distance (WMD)
:PROPERTIES:
:CUSTOM_ID: h2-word-movers-distance-wmd-eab60
:END:

The second problem we need to resolve is the choice of quality metric for
generated adversarial texts, so that we have a scalable way to measure the
effectiveness of our framework.  We employ the Word Mover's Distance
(WMD) cite:kusner2015-from as the metric.  WMD measures the dissimilarity
between two text documents as the minimum amount of distance that the embedded
words of one document need to /travel/ to reach the embedded words of another
document.  WMD can be considered as a special case of Earth Mover's Distance
(EMD) cite:rubner2000-earth.  Intuitively, it quantifies the semantic similarity
between two text bodies.  In this work, WMD is closely related to the ratio of
number of words changed to the sentence length.  However, we plan to extend our
framework with paraphrasing and insertion/deletion, where the sentence length
may change.  In that case, WMD is more flexible and accurate.

* Experiment
:PROPERTIES:
:CUSTOM_ID: h1-experiment-2f800
:END:

# should be in subsec:result-deepfool, placed here for typesetting
#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via DeepFool.  Refer to Section ref:subsec:results for notations and discussions.
#+NAME: fig:textdemo-deepfool
[[file:img/deepfool-eps40.pdf]]

We evaluate our framework on three text classification problems.
Section ref:subsec:dataset details on the data preprocessing.  The adversarial
attacking algorithms which we use are (FGM) cite:goodfellow2014-explaining and
DeepFool cite:moosavi-dezfooli2015-deepfool.  We tried JSMA, however, due to the
mechanism of JSMA, it is not directly applicable in our framework.  We report in
Section ref:subsec:results the original model accuracy, accuracy on adversarial
embeddings, and accuracy on reconstructed adversarial texts in our experiment.
Only a few examples of generated adversarial texts are shown in this paper due
to the space constraint.  The complete sets of adversarial texts under different
parameter settings and the code to reproduce the experiment are available on our
website[fn:1].

Computation-wise, the bottleneck in our framework is the nearest neighbor
search.  Word vector spaces, such as GloVe cite:pennington2014-glove, usually
have millions or billions of tokens embedded in very high dimensions.  The
vanilla nearest neighbor search is almost impractical.  Instead, we employ the
an approximate nearest neighbor (ANN) technique in our experiment.  The ANN
implementation which we use in our experiment is Approximate Nearest Neighbors
Oh Yeah (=annoy=)[fn:2], which is well integrated into =gensim=
cite:rek2010-software package.

** Dataset
:PROPERTIES:
:CUSTOM_ID: h2-dataset-ead0c
:END:

We use three text datasets in our experiments.  The datasets are summarized in
Table ref:tab:datasets.  The last column shows our target model accuracy on
clean test data.

#+ATTR_LaTeX: :booktabs t :width \linewidth
#+CAPTION: Dataset Summary
#+NAME: tab:datasets
| Dataset   | Labels | Training | Testing | Max Length | Accuracy |
|-----------+--------+----------+---------+------------+----------|
| IMDB      |      2 |    25000 |   25000 |        300 |   0.8787 |
| Reuters-2 |      2 |     3300 |    1438 |        100 |   0.9854 |
| Reuters-5 |      5 |     1735 |     585 |        100 |   0.8701 |

*** IMDB Movie Reviews
:PROPERTIES:
:CUSTOM_ID: h3-imdb-movie-reviews-a4a29
:END:

This is a dataset for binary sentiment classification cite:maas2011-learning.
It contains a set of 25,000 highly polar (positive or negative) movie reviews
for training, and 25,000 for testing.  No special preprocessing is used for this
dataset except that we truncate/pad all the sentences to a fixed maximum
length, 400.  This max length is chosen empirically.

*** Reuters
:PROPERTIES:
:CUSTOM_ID: h3-reuters-5b0ea
:END:

This is a dataset of 11,228 newswires from Reuters, labeled over 90 topics.  We
load this dataset through the NLTK cite:bird2009-natural package.  The raw
Reuters dataset is highly unbalanced.  Some categories contain over a thousand
samples, while others may contain only a few.  The problem with such highly
unbalanced data is that the texts that belong to under-populated categories are
almost always get classified incorrectly.  Even though our model may still
achieve high accuracy with 90 labels, it would be meaningless to include these
under-populated categories in the experiment since we are mainly interested in
perturbation of those samples that are already being classified correctly.
Keras[fn:3] uses 46 categories out of 90.  However, the 46 categories are still
highly unbalanced.  In our experiment, we preprocess Reuters and extract two
datasets from it.

**** Reuters-2
:PROPERTIES:
:CUSTOM_ID: h4-reuters-2-6baa5
:END:

It contains two most populous categories, i.e., =acq= and =earn=.  The =acq=
category contains 1650 training samples and 719 test samples.  Over 71%
sentences in the =acq= category have less than 160 tokens.  The =earn= category
contains 2877 training samples and 1087 test samples.  Over 83% sentences in
=earn= category have less then 160 tokens.  In order to balance the two
categories, for =earn=, we use 1650 samples out of 2877 for training, and 719
for testing.  The maximum sentence length of this binary classification dataset
is set to 160.

**** Reuters-5
:PROPERTIES:
:CUSTOM_ID: h4-reuters-5-2388e
:END:

It contains five categories, i.e., =crude=, =grain=, =interest=, =money-fx= and
=trade=.  Similar to Reuters-2, we balance the five categories by using 347
examples (the size of =interest= categories) for each category during training,
and 117 each for testing.  The maximum sentence length is set to 350.

# should be in subsec:result-fgm, placed here for typesetting
#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via FGSM.  Refer to Section ref:subsec:results for notations and discussions.
#+NAME: fig:textdemo-fgsm
[[file:img/fgsm-eps35.pdf]]

** Embedding
:PROPERTIES:
:CUSTOM_ID: h2-embedding-ed890
:END:

Our framework relies heavily on the /size/ and /quality/ of the embedding space.
More semantic alternatives would be helpful to improve the quality of generated
adversarial texts.  As a result, we use the GloVe cite:pennington2014-glove
pre-trained embedding in our experiment.  Specifically, we use the largest GloVe
embedding, =glove.840B.300d=, which embeds 840 billion tokens (approximately 2.2
million cased vocabularies) into a vector space of 300 dimensions.  The value
range of the word vectors are roughly \((-5.161, 5.0408)\).

** Model
:PROPERTIES:
:CUSTOM_ID: h2-model-f41fe
:END:

In this work, we focus on feedforward architectures.  Specifically, we use CNN
model for the classification tasks.  The model structure is summarized in
Figure ref:fig:model-imdb.

#+ATTR_LaTeX: :width \linewidth :placement [!ht]
#+CAPTION: CNN model for text classification.
#+NAME: fig:model-imdb
[[file:img/model-imdb.pdf]]

Where \(B\) denotes batch size, \(L\) the maximum sentence length, \(D\) the
word vector space dimension.  In our experiment, we have \(B=128\), and
\(D=300\) since we are using the pre-trained embedding =glove.840B.300d=.

Note that for models trained for binary classification tasks, DeepFool assumes
the output in the range \([-1, 1]\), instead of \([0, 1]\).  Thus we have two
slightly different models for each of the binary classification task (IMDB and
Reuters-2), one with =sigmoid= output, and the other with =tanh=.  The model
with =tahn= output is trained with Adam cite:kingma2014-adam by minimizing the
mean squared error (MSE), while all the other models are trained with Adam by
minimizing the cross-entropy loss.  Despite the small difference in
architecture, =sigmoid=- and =tanh=-models on the same task have almost
identical accuracy.  As a result, in Table ref:tab:datasets, we report only one
result for IMDB and Reuters-2.

All our models have \(N=256\) and \(M=512\), except for the one with =tanh=
output on the IMDB classification task, in which we have \(N=128\) and
\(M=256\).  The reason that we change to a smaller model is that the larger one
always gets stuck during the training.  We are not yet clear what causes this
problem and why a smaller model helps.

** Results
:PROPERTIES:
:CUSTOM_ID: h2-results-1b24d
:END:

The model accuracy on adversarial embeddings before and after the nearest
neighbor search under different parameter settings are summarized in
Table ref:tab:acc.

In the adversarial text examples, to aid reading, we omit the parts that are not
changed, denoted by \textbf{[\(\boldsymbol\ldots\)]} in the texts.  The
"(\textsc{IMDB})" at the end of each clean text piece denotes the dataset that
this piece of text belongs to.  In addition to Word Mover's Distance (WMD), we
also report the change rate, \(\frac{n}{L}\), where \(n\) is the number of
changed words, \(L\) the sentence length.  The corresponding changed words are
\colorbox{red!10}{highlighted} in the figures.

*** Fast Gradient Method
:PROPERTIES:
:CUSTOM_ID: h3-fast-gradient-method-56aea
:END:

#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via FGVM.  Refer to Section ref:subsec:results for notations and discussions.
#+NAME: fig:textdemo-fgvm
[[file:img/fgvm-eps50.pdf]]

We first evaluate two versions of FGM, i.e., FGSM and FGVM.  Their example
results are shown in Figure ref:fig:textdemo-fgsm and
Figure ref:fig:textdemo-fgvm, respectively.  For FGVM, it was proposed in
cite:miyato2015-distributional to use \(\frac{\nabla L}{\|\nabla L\|_2}\) to
FGVM usually needs much larger noise scaling factor since most gradients are
close to zero.

*** DeepFool
:PROPERTIES:
:CUSTOM_ID: h3-deepfool-c0b1e
:END:

Adversarial examples are shown in Figure ref:fig:textdemo-deepfool.  We
experiment with different overshoot values (also denoted as \epsilon in the
table).  Usually, for images, we tend to use very small overshoot values, e.g.,
1.02, which creates just enough noise to cross the decision boundary.  However,
in our framework, the reconstructing process is a very strong denoising process,
where much of the subtle noise will be smoothed.  To compensate for this, we
experiment with very large overshoot values.  In practice, this works very well.
As we can see, labels are altered by replacing just one word in many cases.

** Discussion
:PROPERTIES:
:CUSTOM_ID: h2-discussion-45d4e
:END:

In contrary to the experiment in cite:liang2017-deep, our framework generates
much better adversarial texts with gradient methods.  One main reason is that
the embedding space preserves semantic relations among tokens.

Based on the generated text samples, DeepFool generates the adversarial texts
with the highest quality.  Our experiment confirms that the DeepFool's strategy
to search for the optimal direction is still effective in text models.  On the
other hand, the strong denoising process will help to smooth unimportant noise.
FGVM is slightly better than FGSM, which is quite similar to what we saw in
Figure ref:fig:mnistdemo.  By using \(\sign\nabla L\), FGSM applies the same
amount of noise to every feature it finds to be important, which ignores the
fact that some features are more important than others.  Since FGVM does not
follow the optimal direction as DeepFool does, it usually needs larger
perturbation.  In other words, compared to DeepFool, FGVM may change more words
in practice.

* Conclusion
:PROPERTIES:
:CUSTOM_ID: h1-conclusion-2763d
:END:

In this work, we proposed a framework to adapt image attacking methods to
generate high-quality adversarial texts in an end-to-end fashion, without
relying on any manually selected features.  In this framework, instead of
constructing adversarials directly in the raw text space, we first search for
adversarial embeddings in the embedding space, and then reconstruct the
adversarial texts via nearest neighbor search.  We demonstrate the effectiveness
of our method on three texts benchmark problems.  In all experiments, our
framework can successfully generate adversarial samples with only a few words
changed.  In addition, we also empirically demonstrate Word Mover's Distance
(WMD) as a valid quality measurement for adversarial texts.  In the future, we
plan to extend our work in the following directions.
1. WMD is demonstrated to be a viable quality metric for the generated
   adversarial texts.  We can employ the optimization and model attacking
   methods by minimizing the WMD.
2. We use a general embedding space in our experiments.  A smaller embedding
   that is trained on the specific task may help to speed up the computation
   needed to reconstruct the texts.

* TODO COMMENT Improvement
:PROPERTIES:
:CUSTOM_ID: h1-improvement-6e209
:END:

[[file:~/Dropbox/dotfiles/emacs.d/data/notes/improve-textadv.org]]

* Reference                                                          :ignore:
:PROPERTIES:
:CUSTOM_ID: h1-reference-34f03
:END:

#+LaTeX: \printbibliography

* Footnotes
:PROPERTIES:
:CUSTOM_ID: h1-footnotes-35904
:END:

[fn:1] https://github.com/gongzhitaao/adversarial-text

[fn:2] https://github.com/spotify/annoy

[fn:3] https://keras.io/

[fn:4] http://www.daviddlewis.com/resources/testcollections/reuters21578/
