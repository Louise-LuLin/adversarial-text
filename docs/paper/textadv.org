#+TITLE: Adversarial Texts with Gradient Methods
#+AUTHOR: Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, Wei-Shinn Ku

#+STARTUP: overview
#+OPTIONS: toc:nil num:t ^:{}
#+OPTIONS: author:nil title:nil date:nil

#+LATEX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [conference,letter,10pt,final,dvipsnames]

#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage[inline]{enumitem}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage[backend=biber]{biblatex}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=basictext,factor=1100,stretch=10,shrink=10]{microtype}

#+LATEX_HEADER: \addbibresource{~/.local/data/bibliography/nn.bib}
#+LATEX_HEADER: \DeclareMathOperator{\sign}{sign}

# title and author
#+BEGIN_EXPORT latex
% This is the real title appearing in the final PDF
\title{Adversarial Texts with Gradient Methods}

\author{
\IEEEauthorblockN{
  Zhitao Gong\IEEEauthorrefmark{1},
  Wenlu Wang\IEEEauthorrefmark{1},
  Bo Li\IEEEauthorrefmark{2},
  Dawn Song\IEEEauthorrefmark{2},
  Wei-Shinn Ku\IEEEauthorrefmark{1}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}
  \texttt{\{gong,wenluwang,weishinn\}@auburn.edu}\\
  Auburn University, Auburn AL, USA}
\IEEEauthorblockA{\IEEEauthorrefmark{2}
  \texttt{\{crystalboli,dawnsong\}@berkeley.edu}\\
  UC Berkeley, Berkeley, CA, USA}
}
#+END_EXPORT

#+LaTeX: \maketitle

* Abstract                                                           :ignore:

#+BEGIN_abstract
Image adversarial samples have been extensively studied in literature.  In this
work, we propose a framework to extend the gradient-based attacking methods on
image model to text model.  The main difficulties for generating adversarial
texts with gradient-based methods are:
#+BEGIN_EXPORT latex
\begin{enumerate*}[label=(\roman*)]
 \item the input space is discrete, which makes it difficult to add small noise
 in the inputs,
 \item the measurement of the adversarial texts quality is difficult.
\end{enumerate*}
#+END_EXPORT
We tackle the first problem by searching for adversarials in the embedding space
and then reconstruct the adversarial texts via nearest neighbor search.  For the
latter problem, we employ the Word Mover's Distance (WMD) to quantify the
adversarial texts' quality.  Through extensive experiment on three datasets,
IMDB movie reviews, Reuters-2 and Reuters-5 newswires, we show that our method
can generate very high quality adversarial texts that are only a few words
different from the original texts.  In our experiment, we successfully
incorporate FGM and DeepFool into our framework.  In addition, we empirically
show that WMD is closely related with the quality of adversarial texts.
#+END_abstract

* Introduction
:PROPERTIES:
:CUSTOM_ID: sec:introduction
:END:

It has been shown that carefully chosen noise may trick the deep neural networks
into wrong predictions with very high confidence cite:szegedy2013-intriguing.
Many followup work proposed cheap yet effective methods to find such adversarial
noise, e.g., fast gradient method (FGM) cite:goodfellow2014-explaining,
Jacobian-based saliency map approach (JSMA) cite:papernot2015-limitations,
DeepFool cite:moosavi-dezfooli2015-deepfool, CW cite:carlini2016-towards, etc.
We have seen that the investigation of adversarial samples for image models has
provided us new perspectives to understand the mechanism of deep neural
networks, e.g., linear hypothesis cite:goodfellow2014-explaining, rethinking
smoothness assumptions cite:szegedy2013-intriguing.  In addition, many
algorithms have been proposed to increase the robustness of the deep models,
e.g., adversarial training cite:kurakin2016-adversarial.  However, most of the
previous work focused on images.  Only a few attempts have been made on text
models.  We think it is worthwhile to investigate adversarial samples for text
models as well.  In this work, we propose a simple and effective framework to
extend the adversarial attacking methods for images to text model.  There are
two major problems we need to resolve before we can plugin the gradient methods
for generating adversarial images.
1. The input space is discrete.  As a result, it is not possible to accumulate
   small noise computed with gradient methods.  They work well with image model
   since the input space is smooth.
2. It is difficult to quantify the quality of adversarial texts.  For images, we
   usually norm distance (e.g., \(L_1\), \(L_2\), \(L_\infty\)), perceptual
   distance cite:li2002-dpf, etc.  While for text, there are no good metrics to
   measure the quality.

In this work, we propose a general framework in which we generate adversarial
texts via gradient attacking method.  We first search for adversarials in the
text embedding space cite:mikolov2013-efficient via gradient methods and then
reconstruct the adversarial texts via nearest neighbor search.  In addition, we
also empirically evaluate using Word Mover's Distance (WMD) cite:kusner2015-from
as a quality measurement for the adversarial texts.  The advantage of our method
is that no manual features are needed.

This article is organized as follows.  We briefly review recent work on
generating adversarial images and texts in Section ref:sec:related-work.  Our
adversarial text framework is detailed in Section ref:sec:our-method.  We
evaluate our method on various text benchmarks and report the results in
Section ref:sec:experiment.  We conclude this article and provide guides for
future work in Section ref:sec:conclusion.

* Related Work
:PROPERTIES:
:CUSTOM_ID: sec:related-work
:END:

The existence of adversarial samples was first discussed
in cite:szegedy2013-intriguing.  There have been abundance of work on attacking
methods to generate adversarial images.  These adversarial images raise security
concern about the wide application of deep neural
networks cite:kurakin2016-adversarial.  As a result, many work have investigated
defense against these adversarial samples.  However, so far as we see in
literature, the attacking is much easier and cheaper than defense.

For notation, \(x\) is the input, \(y\) the output, \(f\) the target model such
that \(y = f(x)\), \(L\) the loss function, \(x^*\) the adversarial sample.  And
we slightly abuse the notation here, \(L_x\) denotes the loss when \(x\) is the
input.

The attacking methods mainly fall into three categories, /gradient attach/,
/optimization attack/ and /model attack/.

#+ATTR_LaTeX: :width \linewidth
#+CAPTION: Random MNIST adversarial images generated via different attacking algorithms.  The upper image in /Clean/ column is the original clean image.   The upper images in the following columns are adversarial images generated by the corresponding attacking algorithm based on the first clean image, respectively.  The lower image in each column is the differece between the adversarial image and the clean image, illustrated in heatmap.  Below each column is the label predicted by the target model, along with probability in paranthesis.
#+NAME: fig:mnistdemo
[[file:img/imgdemo.pdf]]

** Adversarial Image Method
:PROPERTIES:
:CUSTOM_ID: subsec:adversarial-image
:END:

*** Gradient Attack
:PROPERTIES:
:CUSTOM_ID: subsec:gradient-attack
:END:

This class of attacking methods rely on target model gradients, thus requiring
full knowledge of the target model.  Fast gradient method
(FGM) cite:goodfellow2014-explaining and its variants, iterative FGM and
targeted FGM cite:kurakin2016-adversarial, add to the whole image the noise that
is proportional to \(\nabla L_x\) (FGVM) or \(\sign(\nabla L_x)\) (FGSM).
Jacobian-based saliency map approach (JSMA) cite:papernot2015-limitations, on
the contrary, perturbs one pixel at a time.  It chooses the pixel with the
highest saliency score, \(-\nabla y_t\cdot\nabla y_o\) subject to \(\nabla y_t >
0\), where \(y_t\) is the probability for the desired target class, and \(y_o\)
is the probability sum of all other classes.  Intuitively, it tries to increase
the probability for the desired target class while decreasing others.
DeepFool cite:moosavi-dezfooli2015-deepfool iteratively finds the direction in
which we need to /travel/ the minimum distance to cross the decision boundary of
the target model.  Although in non-linear case, the optimality is not
guaranteed, in practice however, DeepFool usually find very subtle noise
compared to other gradient methods.  Figure ref:fig:mnistdemo shows an example
of four gradient methods on MNIST.

*** Optimization Attack
:PROPERTIES:
:CUSTOM_ID: subsec:optimization-attack
:END:

This class of attacks is usually black-box attack, using the target model only
as an oracle.  Generally speaking, this class minimizes \(\|x^* - x\|_p\)
subject to that \(f(x^*)\neq f(x)\), where \(\|\cdot\|_p\) is \(p\)-norm.
cite:szegedy2013-intriguing uses L-BFGS.  CW cite:carlini2016-towards is a
refinement of the above method which minimizes \(\|x^* - x\|_p - L_{x^*}\).
Many advance attacking algorithms have been proposed based on the optimization
formulation.  cite:moosavi-dezfooli2016-universal showed that, instead of
applying different noise to each image, it is possible to apply the same noise,
i.e., the universal perturbation, to different images, such that the resulting
images still trick the target model in most cases.  One-pixel is also shown to
be possible cite:su2017-one.

*** Model Attack
:PROPERTIES:
:CUSTOM_ID: subsec:model-attack
:END:

Similar to above optimization attack, this class also formulate the adversarial
attack as an optimization problem.  The difference is that, instead of
performing the optimization directly, this class trains a separate model to map
the input to noise or adversarial samples.  Adversarial transformation network
(ATN) cite:baluja2017-adversarial trains a separate model \(g\), that minimizes
\(\beta\|x^*-x\|_p + \|f(x^*)-f(x)\|_{p^\prime}\), where \(g(x) = x^*\).
cite:zhao2017-generating searches for adversarials in a random noise space and
reconstructs the adversarial in the input space.  To create the mapping between
input and noise space, the authors employ an autoencoder structure.
Specifically the mapping consists of
#+BEGIN_EXPORT latex
\begin{enumerate*}
 \item an encoder \(G\), a generator network that maps random noise \(z\) to the
 input \(x\), \(G(z) = x\), and
 \item a decoder \(I\) (referred to as \textsl{inverter}), another generator
 network that maps the input to the random noise, \(I(x) = z\).
\end{enumerate*}
#+END_EXPORT
Generative Adversarial Network (GAN) cite:goodfellow2014-generative is used for
both generator networks.  The whole network is trained end-to-end by minimizing
the loss \(\mathbb{E}_x\|G(I(z)) - x\|_p + \lambda\mathbb{E}_z\|I(G(x)) -
z\|_p\).  After the mapping is established, they can search for adversarial in
the noise space and map it back to input space.

** Adversarial Text Method
:PROPERTIES:
:CUSTOM_ID: subsec:adversarial-text
:END:

Almost all the work in the previous section focus on image models.  As we've
discussed, the main problem to extend current method to text models are the
discrete input space and the lack of quality metric.  The aforementioned model
attack cite:zhao2017-generating is able to resolve the first problem since the
noise space is smooth.  However, the problem with their method is that they
cannot directly control the quality of the generated adversarial samples.  As we
have seen in cite:zhao2017-generating, the generated adversarial images on
complex dataset usually have large visual change.

In cite:liang2017-deep, the authors applied FGM directly on character-level
CNN cite:zhang2015-character without much success.

Most work cite:liang2017-deep,samanta2017-towards,jia2017-adversarial on
attacking text models follow a similar strategy,
#+BEGIN_EXPORT latex
\begin{enumerate*}
 \item identify the features (characters, words, sentences) that have the most
 influence on the prediction, and
 \item perturb these features according to \textsl{manually} constructed
 perturbation candidates, or
 \item insert perturbation candidates directly.
\end{enumerate*}
#+END_EXPORT
This strategy is similar to JSMA, in which the pixel with the highest saliency
score is perturbed first.  The Jacobian value \(\nabla f\) or the loss gradient
\(\nabla L\) are usually selected as the measurement for the feature importance.
And the perturbation candidates usually contains typos, synonyms, antonyms,
frequent words in each categories, etc.

A slight different strategy is proposed in cite:jia2017-adversarial.  The
authors add to the samples manually constructed legit distracting sentences,
which add fake information that does not contradicting with the samples.  This
strategy, however, is not scalable.

* Our Method
:PROPERTIES:
:CUSTOM_ID: sec:our-method
:END:

# Should be in experiment section, placed here for typesetting.
#+BEGIN_EXPORT latex
\begin{table*}[ht]
 \caption{\label{tab:acc} Model accuracy under different parameter settings.
   \(\epsilon\) is the noise scale.  We report two accuracy measurement per
   parameter setting in the format \(acc_1/acc_2\), where \(acc_1\) is the model
   accuracy on adversarial embeddings before nearest neighbor search, \(acc_2\)
   the accuracy on adversarial embeddings that are reconstructed by nearest
   neighbor search.  In other words, \(acc_2\) is the model accuracy on
   generated adversarial texts.}
\centering
\begin{tabular}{rl*{5}{c}}
  \toprule
  Method
  & Dataset
  &
  & \multicolumn{4}{c}{\(acc_1/acc_2\)} \\
  \midrule

  \multirow{5}{*}{FGSM}
  &
  & \(\epsilon\) & 0.40 & 0.35 & 0.30 & 0.25 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.1213 / 0.1334 & 0.1213 / 0.1990 & 0.1213 / 0.4074 & 0.1213 / 0.6770 \\
  & Reuters-2 & & 0.0146 / 0.6495 & 0.0146 / 0.7928 & 0.0146 / 0.9110 & 0.0146 / 0.9680 \\
  & Reuters-5 & & 0.1128 / 0.5880 & 0.1128 / 0.7162 & 0.1128 / 0.7949 & 0.1128 / 0.8462 \\
  \cmidrule(lr){1-7}

  \multirow{5}{*}{FGVM}
  &
  & \(\epsilon\) & 15 & 30 & 50 & 100 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.6888 / 0.8538 & 0.6549 / 0.8354 & 0.6277 / 0.8207 & 0.5925 / 0.7964 \\
  & Reuters-2 & &  0.7747 / 0.7990 & 0.7337 / 0.7538 & 0.6975 / 0.7156 & 0.6349 / 0.6523 \\
  & Reuters-5 & &  0.5915 / 0.7983 & 0.5368 / 0.6872 & 0.4786 / 0.6085 & 0.4000 / 0.5111\\
  \cmidrule(lr){1-7}

  \multirow{5}{*}{DeepFool}
  &
  & \(\epsilon\) & 20 & 30 & 40 & 50 \\
  \cmidrule(r){3-7}
  & IMDB      & & 0.5569 / 0.8298 & 0.5508 / 0.7225 & 0.5472 / 0.6678 & 0.5453 / 0.6416 \\
  & Reuters-2 & & 0.4416 / 0.6766 & 0.4416 / 0.5236 & 0.4416 / 0.4910 & 0.4416 / 0.4715 \\
  & Reuters-5 & & 0.1163 / 0.4034 & 0.1162 / 0.2222 & 0.1162 / 0.1641 & 0.1162 / 0.1402 \\
  \bottomrule
\end{tabular}
\end{table*}
#+END_EXPORT

As we seen, all the previous adversarial text attacking methods rely on
/manually/ selected, and /task-dependent/ features.  In this section, We propose
a general framework that generates high quality adversarial texts without human
intervention.

The general idea is simple.  We first search for adversarials in the text
embedding space via gradient methods, and then reconstruct the adversarial
sentence via nearest neighbor search.  Searching for adversarials in the
embedding space is similar to searching for adversarial images in the raw input
space.  However, the problem for adversarial embeddings is that the perturbed
word vectors are rarely meaningful.  In other word, they do not correspond to
any words in most cases.  In order to reconstruct the perturbed sentence, we use
nearest neighbor search to round the perturbed vectors to nearest word vectors.
This framework builds upon the following observations.
1. The input features (pixels, words) that are relatively more important for the
   final predictions will receive more noise, while others relatively less
   noise.  The is actually the motivation behind many of the adversarial image
   attacking methods.  For example, in Figure ref:fig:mnistdemo, not every pixel
   is perturbed.
2. The embedded word vectors preserve the subtle semantic relationships among
   words cite:mikolov2013-efficient,mikolov2013-distributed.  For example, the
   result of the vector calculation =vec("Madrid") - vec("Spain") +
   vec("France")= is closer to =vec("Paris")= than to any other word
   vectors cite:mikolov2013-linguistic.  This assures that our method replaces
   the victim words with a semantically related one rather than a random one.

Most of the attacking algorithms that apply to image models are applicable in
our framework, e.g, FGM cite:goodfellow2014-explaining,
DeepFool cite:moosavi-dezfooli2015-deepfool, etc.  JSMA is an /exception/ which
we will discussed in detail in Section ref:subsec:results.

* Word Mover's Distance (WMD)

The Word Mover's Distance (WMD) cite:kusner2015-from measures the dissimilarity
between two text documents as the minimum amount of distance that the embedded
words of one document need to /travel/ to reach the embedded words of another
document.  WMD can be considered as a special case of Earth Mover's Distance
(EMD) cite:rubner2000-earth.

* Experiment
:PROPERTIES:
:CUSTOM_ID: sec:experiment
:END:

We evaluate our framework on three text classification problems.
Subsection ref:subsec:dataset details on the preprocessing steps.  The
adversarial attacking algorithms we use are (FGM) cite:goodfellow2014-explaining
and DeepFool cite:moosavi-dezfooli2015-deepfool.  We tried JSMA, however, due to
the mechanism of JSMA, it is not very successful.  We report in
Subsection ref:subsec:results the original model accuracy, accuracy on
adversarial embeddings, and accuracy on reconstructed adversarial texts in our
experiment.  Only a few examples of generated adversarial texts are shown in
this paper due to the space constraint.  The complete sets of adversarial tests
under different parameter settings and the code to reproduce the experiment are
available on our website[fn:1].

Computation-wise, the bottleneck in our framework is the nearest neighbor
search.  Word vector spaces, such as GloVe cite:pennington2014-glove, usually
have millions or billions of tokens in very high dimension.  The vanilla nearest
neighbor search is almost impractical.  Instead, we employ the approximate
nearest neighbor (ANN) techniques in our experiment.  The approximate nearest
neighbor implementation we use in our experiment is Approximate Nearest
Neighbors Oh Yeah (=annoy=)[fn:2], which is well integrated in =gensim=
cite:rek2010-software package.

** Dataset
:PROPERTIES:
:CUSTOM_ID: subsec:dataset
:END:

We use three text datasets in our experiments.  The datasets are summarized in
Table ref:tab:datasets.  The last column shows our target model accuracy on
clean test data.

#+ATTR_LaTeX: :booktabs t :width \linewidth
#+CAPTION: Dataset Summary
#+NAME: tab:datasets
| Dataset   | Labels | Training | Testing | Max Length | Accuracy |
|-----------+--------+----------+---------+------------+----------|
| IMDB      |      2 |    25000 |   25000 |        400 |   0.8787 |
| Reuters-2 |      2 |     3300 |    1438 |        160 |   0.9854 |
| Reuters-5 |      5 |     1735 |     585 |        350 |   0.8701 |

#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via DeepFool.  Refer to Subsection ref:subsec:results for notations and discussions.
#+NAME: fig:textdemo-deepfool
[[file:img/deepfool-eps40.pdf]]

*** IMDB Movie Reviews
:PROPERTIES:
:CUSTOM_ID: subsec:data-imdb
:END:

This is a dataset for binary sentiment classification cite:maas2011-learning.
It contains a set of 25,000 highly polar (positive or negative) movie reviews
for training, and 25,000 for testing.  No special preprocessing is used for this
dataset except that we truncate/pad all the sentences to a fixed maximum
length, 400.

*** Reuters
:PROPERTIES:
:CUSTOM_ID: subsec:data-reuters
:END:

This is a dataset of 11,228 newswires from Reuters, labeled over 90 topics.  We
load this dataset through the NLTK cite:bird2009-natural package.  The raw
Reuters dataset is highly unbalanced.  Some categories contain over a thousand
samples, while others may contain only a few.  The problem with such highly
unbalanced data is that the texts that belong to under-populated categories are
almost always get classified incorrectly.  Even though our model may still
achieve high accuracy with 90 labels, it would be meaningless to include these
texts in the experiment since we are mainly interested in perturb those that can
already be classified correctly.  Keras[fn:3] uses 46 categories out of 90.
However, the 46 categories are still highly unbalanced.  In our experiment, we
preprocess Reuters and extract two datasets from it.

**** Reuters-2
:PROPERTIES:
:CUSTOM_ID: subsec:data-reuters-2
:END:

It contains two most populous categories, i.e., =acq= and =earn=.  The =acq=
category contains 1650 training samples and 719 test samples.  Over 71%
sentences in =acq= category have less then 160 tokens.  The =earn= category
contains 2877 training samples and 1087 test samples.  Over 83% sentences in
=earn= category have less then 160 tokens.  In order to balance the two
categories, for =earn=, we use 1650 samples out of 2877 for training, and 719
for testing.  The maximum sentence length of this binary classification dataset
is set to 160.

**** Reuters-5
:PROPERTIES:
:CUSTOM_ID: subsec:data-reuters-5
:END:

It contains five categories, i.e., =crude=, =grain=, =interest=, =money-fx= and
=trade=.  Similar to above Reuters-2, we balance the five categories by use 347
examples (the size of =interest= categories) for each category during training,
and 117 each for testing.  The maximum sentence length is set to 350.

** Embedding
:PROPERTIES:
:CUSTOM_ID: subsec:embedding
:END:

Our framework relies heavily on the /size/ and /quality/ of the embedding space.
As a result, we use the GloVe cite:pennington2014-glove pre-trained embedding in
our experiment.  Specifically, we use the largest GloVe embedding,
=glove.840B.300d=, which embeds 840 billions tokens (approximately 2.2 million
cased vocabularies) into a vector space of 300 dimension.  The value range of
the word vectors are roughly \((-5.161, 5.0408)\).

** Model
:PROPERTIES:
:CUSTOM_ID: subsec:model
:END:

In this work, we focus on feedforward architectures.  Specifically, we use CNN
model for the classification tasks.  The model structure is summarized in
Figure ref:fig:model-imdb.

#+ATTR_LaTeX: :width \linewidth :placement [!ht]
#+CAPTION: CNN model for text classification.
#+NAME: fig:model-imdb
[[file:img/model-imdb.pdf]]

Where \(B\) denotes batch size, \(L\) the maximum sentence length, \(D\) the
word vector space dimension.  In our experiment, we have \(B=128\), and
\(D=300\) since we are using the pre-trained embedding =glove.840B.300d=.

Note that for models trained for binary classification tasks, DeepFool assumes
the output in the range \([-1, 1]\), instead of \([0, 1]\).  Thus we have two
slightly different models for each of the binary classification task, IMDB and
Reuters-2, one with =sigmoid= output, and the other with =tanh=.  The model with
=tahn= output is trained with Adam cite:kingma2014-adam by minimizing the mean
squared error (MSE), while all the other models are trained with Adam by
minimizing the cross entropy loss.  Despite the small difference in
architecture, =sigmoid=- and =tanh=-models on the same task have almost
identical accuracy.  As a result, in Table ref:tab:datasets, we report only one
result for IMDB and Reuters-2.

All our models have \(N=256\) and \(M=512\), except for the one with =tanh=
output on the binary classification task IMDB, in which we have \(N=128\) and
\(M=256\).  The reason that we change to a smaller model is that the larger one
always gets stuck during the training.  We are not yet clear what causes this
problem and why a smaller model helps.

** Results
:PROPERTIES:
:CUSTOM_ID: subsec:results
:END:

The model accuracy on adversarial embeddings before and after the nearest
neighbor search under different parameter settings are summarized in
Table ref:tab:acc.

In the adversarial texts examples, to aid reading, we omit the parts that are
not changed, denoted by \textbf{[\(\boldsymbol\ldots\)]} in the texts.  The
"(\textsc{IMDB})" at the end of each clean text denotes the dataset that this
piece of text belongs to.  In addition to Word Mover's Distance (WMD), we also
report the change rate, \(\frac{n}{L}\), where \(n\) is the number of changed
words, \(L\) the sentence length.  The corresponding changed words are
\colorbox{red!10}{highlighted} in the figures.

*** Fast Gradient Method
:PROPERTIES:
:CUSTOM_ID: subsec:result-fgm
:END:

We first evaluate in our framework the two version of FGM, i.e., FGSM and FGVM,
Their results are show in Figure ref:fig:textdemo-fgsm and
Figure ref:fig:textdemo-fgvm, respectively.  FGVM usually needs much larger
noise scaling factor since some gradients are close to zero.

#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via FGSM.  Refer to Subsection ref:subsec:results for notations and discussions.
#+NAME: fig:textdemo-fgsm
[[file:img/fgsm-eps35.pdf]]

*** DeepFool
:PROPERTIES:
:CUSTOM_ID: subsec:result-deepfool
:END:

Adversarial examples are shown in Figure ref:fig:textdemo-deepfool.  We
experiment with different overshoot values (denoted as \epsilon in the table).
Usually for images, we tend to use very small overshoot values, e.g., 1.02,
which creates just enough noise to cross the decision boundary.  However, in our
framework, the reconstructing process is a very strong denoising process, where
much of the subtle noise is lost.  To overcome this problem, we experiment with
very large overshoot values.  In practice, this works very good.  In many cases,
only one word is changed.

** Discussion
:PROPERTIES:
:CUSTOM_ID: subsec:discussion
:END:

#+ATTR_LaTeX: :float multicolumn :width \textwidth
#+CAPTION: Adversarial texts generated via FGVM.  Refer to Subsection ref:subsec:results for notations and discussions.
#+NAME: fig:textdemo-fgvm
[[file:img/fgvm-eps50.pdf]]

Based on the generated text samples, DeepFool generates the adversarial texts
with highest quality.  In many cases, DeepFool can change the labels by changing
only one word.  FGVM is slight better than FGSM.  Actually, this is as
expected.  In Figure ref:fig:mnistdemo, FGSM tends to generate noise that affect
most part of the image, and DeepFool chooses the features to perturb more
cleverly and the noise are very subtle.  While FGVM tends to stay in the middle.

As a matter of fact, our framework also follows the strategy outlined in
Section ref:sec:related-work.  The difference is that the important features and
perturbation candidates are selected automatically by the attacking algorithm.

* Conclusion
:PROPERTIES:
:CUSTOM_ID: sec:conclusion
:END:

In this work we proposed a framework to adapt image attacking method to generate
adversarial texts, by searching for adversarials in embedding space and
reconstructing the adversarial texts via nearest neighbor search.  We
demonstrate the effectiveness of our method on two text benchmark problems.  In
addition, we also empirically demonstrate the Word Mover's Distance (WMD) as a
valid quality measurement for adversarial text.  There are several directions to
extend our work in the future.
1. Adapt our framework to recurrent models.
2. Improve the evaluation metric to take into consideration the structure
   information besides word-wise similarity.

* Reference                                                          :ignore:

#+LaTeX: \printbibliography

* Footnotes

[fn:1] [[http://gongzhitaao.org/adversarial-text]]

[fn:2] https://github.com/spotify/annoy

[fn:3] https://keras.io/

[fn:4] http://www.daviddlewis.com/resources/testcollections/reuters21578/
